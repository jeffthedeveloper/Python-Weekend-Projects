{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fda2239",
   "metadata": {},
   "source": [
    "\n",
    "# Big Data Best Practices - Complete Project Notebook\n",
    "### **by JeffTheDeveloper**  \n",
    "##### *Inspired by Néstor Nicolás Campos Rojas' methodologies*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee21887",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "## 1. Project Setup & File Structure\n",
    "# Creates the complete folder structure and generates sample datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a59f1d",
   "metadata": {},
   "source": [
    "# If doesn't work try this one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e55cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de todas as dependências em uma única célula\n",
    "!pip install pandas==1.5.0 numpy==1.21.0 jupyter==1.0.0 ipython==8.0.0 pyarrow==8.0.0 tqdm==4.65.0 python-dotenv==0.21.0\n",
    "!pip install jupyter-contrib-nbextensions==0.5.1 jupyter-nbextensions-configurator==0.4.1\n",
    "\n",
    "# Verificação das versões instaladas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython\n",
    "\n",
    "print(\"\\n✅ Dependências instaladas com sucesso:\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"IPython: {IPython.__version__}\")\n",
    "print(\"Pronto para executar o projeto Big Data Best Practices!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3469fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependências avançadas (descomente se necessário)\n",
    "# !pip install pyspark==3.3.0 databricks-connect==10.4 plotly==5.11.0 ipywidgets==8.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da190972",
   "metadata": {},
   "source": [
    "**Execute esta célula uma vez no início do notebook**\n",
    "\n",
    "Se estiver no Databricks, substitua por:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c168071",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdbutils\u001b[49m.library.installPyPI(\u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m1.5.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m dbutils.library.installPyPI(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "dbutils.library.installPyPI(\"pandas\", version=\"1.5.0\")\n",
    "dbutils.library.installPyPI(\"pyarrow\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ba519",
   "metadata": {},
   "source": [
    "\n",
    "**Para ambientes com restrições de internet, use:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44268ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install --no-deps pandas numpy  # Ignora dependências secundárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d18c2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Create folder structure\n",
    "folders = [\n",
    "    'big-data-best-practices/databricks_notebooks',\n",
    "    'big-data-best-practices/sample_data/transactions',\n",
    "    'big-data-best-practices/docs'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"Created directory: {folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42d6dd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Sample Data Generation\n",
    "# Creates realistic datasets for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Transaction data (streaming)\n",
    "transactions = [\n",
    "    {\"transaction_id\": f\"T{1000+i}\", \"amount\": np.random.randint(10,500), \n",
    "     \"currency\": np.random.choice([\"USD\", \"BRL\", \"EUR\"]), \"timestamp\": pd.Timestamp.now().isoformat()}\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "with open('big-data-best-practices/sample_data/transactions/stream_1.json', 'w') as f:\n",
    "    json.dump(transactions[:25], f)\n",
    "    \n",
    "with open('big-data-best-practices/sample_data/transactions/stream_2.json', 'w') as f:\n",
    "    json.dump(transactions[25:], f)\n",
    "\n",
    "# Reference data\n",
    "reference_data = pd.DataFrame({\n",
    "    \"currency\": [\"USD\", \"BRL\", \"EUR\"],\n",
    "    \"exchange_rate\": [1.0, 5.20, 0.95],\n",
    "    \"risk_level\": [\"Low\", \"High\", \"Medium\"]\n",
    "})\n",
    "reference_data.to_csv('big-data-best-practices/sample_data/reference_data.csv', index=False)\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "display(reference_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e64b2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Notebook Creation\n",
    "# Generates the Databricks notebooks with best practices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7deb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "notebooks = {\n",
    "    \"1_Data_Ingestion.ipynb\": \"\"\"{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Data Ingestion (Best Practices)\\\\n\",\n",
    "    \"From Néstor Campos' Big Data Course\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# AutoLoader configuration\\\\n\",\n",
    "    \"df = (spark.readStream\\\\n\",\n",
    "    \"  .format(\\\\\"cloudFiles\\\\\")\\\\n\",\n",
    "    \"  .option(\\\\\"cloudFiles.format\\\\\", \\\\\"json\\\\\")\\\\n\",\n",
    "    \"  .load(\\\\\"/mnt/sample_data/transactions\\\\\"))\"\n",
    "   ]\n",
    "  }\n",
    " ]\n",
    "}\"\"\",\n",
    "    \"2_Data_Transformation.ipynb\": \"\"\"{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Data Transformation\\\\n\",\n",
    "    \"## Currency Normalization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pyspark.sql.functions import col, when\\\\n\",\n",
    "    \"\\\\n\",\n",
    "    \"df_normalized = (df\\\\n\",\n",
    "    \"  .join(spark.table(\\\\\"reference_data\\\\\"), \\\\\"currency\\\\\")\\\\n\",\n",
    "    \"  .withColumn(\\\\\"amount_usd\\\\\", col(\\\\\"amount\\\\\") * col(\\\\\"exchange_rate\\\\\")))\"\n",
    "   ]\n",
    "  }\n",
    " ]\n",
    "}\"\"\"\n",
    "}\n",
    "\n",
    "for filename, content in notebooks.items():\n",
    "    path = f\"big-data-best-practices/databricks_notebooks/{filename}\"\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(f\"Created notebook: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594909e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Security Configurations\n",
    "# Advanced security settings for production environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daded939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#security_config = \"\"\"# Databricks Security Best Practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522e25a",
   "metadata": {},
   "source": [
    "\n",
    "1. Cluster-Level:\n",
    "   - Enable Table Access Control\n",
    "   - Use Single User mode for sensitive jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c193f",
   "metadata": {},
   "source": [
    "\n",
    "2. Data Protection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df008df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.preview.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.repl.allowedLanguages\", \"python,sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eda198",
   "metadata": {},
   "source": [
    "\n",
    "3. IAM Roles:\n",
    "   - Minimum privilege principle\n",
    "   - SCIM provisioning for user management\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fa5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('big-data-best-practices/docs/security_configurations.md', 'w') as f:\n",
    "    f.write(security_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c01e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Create Downloadable ZIP\n",
    "# Packages all project files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53874c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip():\n",
    "    zip_path = 'big-data-best-practices.zip'\n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for root, dirs, files in os.walk('big-data-best-practices'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "    return zip_path\n",
    "\n",
    "zip_file = create_zip()\n",
    "print(f\"Project ZIP created: {zip_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a72720",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Databricks Setup Walkthrough\n",
    "# Embedded video guide for Databricks Community setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a12b6e",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<h3>Databricks Setup Video Guide</h3>\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6t_rcDU8e5w\" \n",
    "title=\"YouTube video player\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "<h4>Step-by-Step:</h4>\n",
    "<ol>\n",
    "  <li>Go to <a href=\"https://community.cloud.databricks.com/\" target=\"_blank\">Databricks Community</a></li>\n",
    "  <li>Sign up with GitHub</li>\n",
    "  <li>Create cluster (11.3 LTS runtime)</li>\n",
    "  <li>Import notebooks from ZIP</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd2208",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. Key Skills Demonstrated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = {\n",
    "    \"Cluster Management\": [\"Auto-scaling\", \"Library installation\", \"IAM integration\"],\n",
    "    \"Data Processing\": [\"Delta Lake\", \"Streaming\", \"Koalas optimization\"],\n",
    "    \"Security\": [\"Table ACLs\", \"Data encryption\", \"Audit logging\"]\n",
    "}\n",
    "\n",
    "pd.DataFrame.from_dict(skills, orient='index').transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b6cf8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. Complete Project Execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb881c5",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background: #f8f9fa; padding: 20px; border-radius: 5px;\">\n",
    "  <h3>Ready to Execute!</h3>\n",
    "  <p>After setting up Databricks:</p>\n",
    "  <pre><code># Run this in your first notebook cell\n",
    "dbutils.library.installPyPI(\"koalas\")\n",
    "dbutils.library.restartPython()</code></pre>\n",
    "  \n",
    "  <p>Download project files: <a href=\"./big-data-best-practices.zip\" download>big-data-best-practices.zip</a></p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2589fb2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 9. Additional Resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8636df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resources = {\n",
    "    \"Course\": [\"Big Data Best Practices - Néstor Campos (Coursera)\"],\n",
    "    \"Documentation\": [\"Databricks Docs\", \"Spark API Reference\"],\n",
    "    \"Community\": [\"Databricks Forum\", \"Stack Overflow #spark\"]\n",
    "}\n",
    "\n",
    "pd.DataFrame.from_dict(resources, orient='index').transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
